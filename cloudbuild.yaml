steps:
  # Build the container image
  - name: "gcr.io/cloud-builders/docker"
    args: ["build", "-t", "gcr.io/panprices/test-crawlee", "."]
  # Push the container image to Container Registry
  - name: "gcr.io/cloud-builders/docker"
    args: ["push", "gcr.io/panprices/test-crawlee"]
  # Deploy container image to Cloud Run
  - name: "gcr.io/google.com/cloudsdktool/cloud-sdk"
    entrypoint: gcloud
    args:
      - run
      - deploy
      - test-crawlee
      - --image
      - gcr.io/panprices/test-crawlee
      - --region
      - europe-west1
      - --platform
      - managed
      - --cpu=1
      - --memory=4Gi
      - --concurrency=1 # should only run 1 scraper at a time
      - --timeout=3600s # max for Cloud Run. Note that the caller (Cloud Task or Pub/sub) have a lower limit
      - --set-env-vars=PANPRICES_ENVIRONMENT=production
      - --set-env-vars=SHELF_ANALYTICS_SCHEDULE_PRODUCT_SCRAPE_TOPIC=trigger_schedule_product_scrapes
      - --set-env-vars=SHELF_ANALYTICS_UPDATE_PRODUCTS_TOPIC=trigger_shelf_analytics_update_products
      - --set-env-vars=CRAWLEE_AVAILABLE_MEMORY_RATIO=1
images:
  - gcr.io/panprices/test-crawlee
